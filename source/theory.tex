\section{Теоретическая часть}
\subsection{Случайные события и случайная величина}

{\it Статистическое испытание} - это наблюдение, производимое при неизменном комплексе контролируемых условий.

Всякий исход испытания называется {\it случайным событием}. 

В нашем опыте случайным событием является попадание зернышка в какую-либо из ячеек.

Случайные события принято описывать количественно с помощью случайных величин. Например, номер ячейки \textbf{n}, в которую попало зернышко, время падения в ячейку или пройденный до ячейки путь - это случайные величины, относящиеся к рассматриваемому случайному событию.

\subsubsection{Свойство статистической устойчивости. Относительная частота и вероятность события}

Ключевое понятие вероятности случайного события опирается на свойство \textit{статистической устойчивости}, которое поясним на примере.

Пусть зёрнышко брошено на доску Гальтона \textit{N} раз. Обозначим $N_k$ число испытаний, в которых зерно попало в ячейку с заданным номером \textit{k}(или же один раз брошено \textit{N} одинаковых зёрен, тогда $N_k$ - число зёрен в \textit{k}-й ячейке). Отношение $P^*(k,N) = \frac{N_k}{N}$ называется \textit{относительной частотой} события, заключающегося в попадании зерна в ячейку с номером \textit{k} в серии из \textit{N} испытаний. По-другому можно сказать, что это относительная частота того, что случайный номер ячейки \textit{n} примет значение \textit{k}.

Относительная частота - случайная величина. Но если провести $N$ одинаковых испытаний, то окажется, что чем больше $N$, тем меньше относительная частота зависит от $N$. Это свойство называется статистической устойчивостью относительной частоты появления случайного события. Именно статистическая устойчивость позволяет построить для случайных явлений и величин теорию, предсказывающую результаты многократно воспроизводимых(при одинаковых условиях) испытаний. Статистическая устойчивость - частный случай появления основного статистического закона, который называется законом больших чисел.

На математическом языке тот факт, что с увеличением $N$ относительная частота становится всё менее случайной, записывается в виде
\begin{align}
	\lim\limits_{N\to\infty}P^\ast(k,N) = \lim\limits_{N\to\infty}\frac{N_k}{N} = P(k)
\end{align}

Детерминорованную величину $P(k)$ называют вероятностью случайного события. В данном случае событие состоит в попадании зерна в $k$-ю ячейку, в то же время можно сказать, что $P(k)$ есть вероятность того, что случайная величина $n$ равна $k$.

\subsubsection{Дискретные и непрерывные случайные величины}
Случайною величину $X$, которая может принимать ограниченное или счётное число значений $\{x_1, x_2,\dots, x_n,\dots\}$, называют \textit{дискретной}. В нашем случае дискретной величиной является номер ячейки.
Величины, принимающие непрерывный ряд значений(например, время падения зерна в ячейку), называют \textit{непрерывными} случайными величинами.

\subsection{Закон распределения случайной величины}
\subsubsection{Закон распределения дискретной случайной величины}
Все свойства дискретной случайной величины определяются вероятностью возможных значений:
\begin{align*}
	P(k_1) = p_1, P(k_2) = p_2, \dots, P(k_n) = p_n, \dots
\end{align*}
 
Если набор значений невелик, то составляют таблицу, первая строка которой включает все значения случйной величины, а вторая - их вероятности. При этом говорят, что задан \textit{закон распределения} случайной дискретной величины. Тот же закон можно представить графически, откладывая по оси абсцисс значения, которые принимает случайная величина, а на оси ординат - их вероятности. 
 
\subsubsection{Интегральная и дифференциальная функции распределения}
Запись распределения случайных величин в виде таблицы неудобна в аналитических расчётах. Удобнее использовать \textit{функцию распределения}. По определению \textit{интегральная функция распределения}
\begin{align} \label{integr_func_raspr}
	F(x) = P(X < x)
\end{align}

равна вероятности того, что случайна величина $X$ принимает значение, меньшее наперёд заданного $x$. Интегральная функция распределения обладает следующими свойствами:
\begin{enumerate}
	\item $F(x)$ - неубывающая функция $x$, определённая на всей оси $x\in(-\infty,\infty)$ и принимающая значения в интервале $[0, 1]$.
	\item {Наименьшее значение функции $F(x)$ достигается при $x = -\infty$, а наибольшее - при $x = \infty$.
	\begin{align}
		F(-\infty)=0, \qquad F(\infty) = 1.
	\end{align}
	}
\end{enumerate}

Применительно к дискретной случайной величине $F(x)$ представляет собой кусочно-постоянную функцию, терпящую скачки в точках разрешённых значений $x_k$ случайной величины $X$:

\begin{align} \label{raspr_diskr}
	F(x) = \sum_{k}{p_k \chi (x - x_k).}
\end{align}
В записи (\ref{raspr_diskr}) использована $\chi$ - единичная функция 
\begin{align}
	\chi(x) = \left \{
	\begin{aligned}
		0, \qquad x \leq 0 \\
		1, \qquad x > 0
	\end{aligned} \right.
\end{align}

так что величина скачка равна вероятности $p_k$.

Интегральная функция распределения непрерывной случайной величины является гладкой монотонно возрастающей.

Наряду с интегральной функцией распределения часто используют и \textit{дифференциальную функцию распределения}, или \textit{плотность вероятностей}, по определению равную 
\begin{align} \label{opr_dif_func_raspr}
	W(x) = \frac{dF}{dx}
\end{align}

 Если интервал $\Delta x$ достаточно мал, то из (\ref{opr_dif_func_raspr}) и (\ref{integr_func_raspr})  следует, что величина $W(x)\Delta x$ будет приближённо равна вероятности попадания случайной величины $X$ в интервал значений $\Delta x$. Поэтому с помощью плотности вероятностей можно найти вероятность попадания случайной величины $X$ в любой наперёд заданный интервал $[a, b)$:
 \begin{align}
 	P(a \leq X < b) = \int_{a}^{b}{W(x)dx}
 \end{align}
 
 В частности, отсюда следует явное выражение для интегральной функции распределения через плотность вероятностей 
 \begin{align} \label{inter_func_raspr_cherez_plot_ver}
 	F(x) = P(X < x) = \int_{-\infty}^{x}{W(x)dx}
 \end{align}
 
 Перечислим общие свойства плотности вероятностей:
 \begin{enumerate}
 	\item Из (\ref{opr_dif_func_raspr}) и (\ref{integr_func_raspr}) видно, что плотность вероятностей имеет размерность, обратную размерности случайной величины $X$.
 	\item 
 	{ 
 		Плотность вероятностей неотрицательна:
 		
 		\begin{align}
 			W(x) \geq 0.
 		\end{align}
 	}
 	\item 
 	{
 		Для плотность вероятностей выполнено \textit{условие нормировки}, которое получим, устремив в \eqref{inter_func_raspr_cherez_plot_ver} $x$ к бесконечности:
 		
 		\begin{align} \label{usl_normirovki}
 			\int_{-\infty}^{\infty}{W(z)dz} = 1
 		\end{align}
 	}
 \end{enumerate}
 
 
 \subsubsection{Среднее значение и дисперсия}
 Пусть дискретная случайная величина $X$ в $N$ независимых испытаниях принимает значения $x_1,x_2, \dots, x_N$. Тогда среднее значение (его будем обозначать чертой сверху) равно
 \begin{align} \label{sr_znach}
 	\overline X = \frac{1}{N} \sum_{i=1}^{N}X_i.
 \end{align}
 
 Здесь $X_i$ - исход $i-$го испытания. Вычислим предел среднего арифметического при безграничном увеличении $N$. Для этого перегруппируем слагаемые считая, что значения $x_k$ выпадают $N_k$ раз. Тогда
 \begin{align}
 	\overline X = \sum_{k}{x_k \frac{N_k}{N}}.
 \end{align}
 
 При большом $N$ каждая дробь под знаком суммы даёт вероятность $p_k$, в итоге 
 \begin{align} \tag{12a} \label{x_sr_sum}
 	\overline X = \sum_{k}{x_k p_k} 
 \end{align}
 
 Равенство \eqref{x_sr_sum} является определением среднего значения дискретной случайной величины. Его ещё называют \textit{математическое ожидание} и обозначают $E_x$.
 Математическое ожидание (среднее значение) непрерывной случайной величины вычисляется с помощью плотности вероятностей:
  \begin{align*} \tag{12b} \label{x_sr_int}
 	\overline X = \int_{ -\infty }^{ \infty } (x W(x) dx)
 \end{align*}
 
 Ещё более информативной, чем математическое ожидание является \textit{дисперсия} случайной величины $D_x$, по определению равная:
 \begin{align} \label{13}
 	D_x = \overline{(X - \overline{X})^2}
 \end{align}

Из определения среднего значения следует, что дисперсия дискретной случайной величины вычисляется по формуле:
\begin{align} \tag{13a}
	D_x = \sum_{k} { (x_k - \overline{X})^2 p_k } = \sum_{k} { x_k^2 p_k - \overline{X}^2 },
\end{align}

а непрерывной - по формуле
\begin{align} \tag{13b}
	D_x = \int_{-\infty}^{\infty} { (x - \overline{X})^2 \, W(x) dx } = \int_{-\infty}^{\infty} { x^2 W(x) dx } - \overline{X}^2
\end{align}

По смыслу математическое ожидание есть постоянная составляющая случайной величины $X$, а дисперсия служит количественной мерой случайности - разброса $X$ вокруг среднего. В частности, детерминированная величина совпадает со своим средним, а её дисперсия равна нулю.

В инженерных приложениях, где приходится иметь дело с размерными величинами, удобнее использовать не дисперсию, а \textit{среднеквадратичное отклонение} случайной величины от среднего:
\begin{align}
	\sigma_x = \sqrt{D_x}
\end{align}

По-другому эту величину называют \textit{стандартным отклонением} ли просто \textit{стандартом} случайной величины $X$.

Случайные отклонения величины от среднего значения называются \textit{флуктуациями}. Наиболее показательной характеристикой таких отклонения является \textit{относительная флуктуация}, по определению равная
\begin{align}
	\eta = \frac{\sigma_x}{\overline{X}}
\end{align}

Пусть некоторый опыт повторяется независимо $N$ раз, а вероятность наступления события $A$ не зависит от номера опыта и равна $p$ (например, в опытах с доской Гальтона событие $A$ состоит в том, что зёрнышко попадает в ячейку с заранее заданным номером). Если $X$ - число наступлений события $A$ в серии из $N$ опытов, то можно показать:
\begin{align}
	\overline{X} = N p, && \sigma_x = \sqrt{N p (1 - p)}, && \eta = \sqrt{\frac{1 - p}{N p}}
\end{align}

\subsection{Закон распределения для доски Гальтона}

В опытах с доской Гальтона при большом числе частиц вероятность $P(k)$ пропорциональна высоте столбика в ячейке $k$. Колоколообразная кривая, которую можно провести через точки на графике, будет иметь ту же форму, что и холмик, образованный зёрнами  ячейках. Эту кривую называют \textit{кривой вероятностей}.

Обозначим $\overline{k}$ номер средней ячейки, над которой находится воронка. Средняя ячейка доски Гальтона оказывается наиболее вероятной: вероятность $P(\overline{k})$ попадания в неё максимальна. Оказывается, при достаточно большом числе ячеек вероятность $P(k)$ приближённо выражается формулой
\begin{align} \label{ver_formul}
	P(k) = P(\overline{k})
	\exp \left( - \frac{(k - \overline{k})^2}{2 \sigma_x^2} \right)
\end{align}

Чтобы выяснить влияние $\sigma_x$ на вид распределения, положим в формуле \eqref{ver_formul} значения $k$ равными $k_1 = \overline{k} + \sigma_k$ и $k_2 = \overline{k} - \sigma_k$. Формула \eqref{ver_formul} даёт тогда 
\begin{align*}
	P(k_1) = P(k_2) = \frac{P(\overline{k})}{\sqrt{e}} 
\end{align*}

Это значит, что $2\sigma_k = k_2 - k_1$ равняется ширине кривой вероятностей, измеренной на на уровне ${P(\overline{k})}/{\sqrt{e}}$, т.е. стандарт характеризует величину случайных отклонений от среднего значения.

Установим значение $P(\overline{k})$. Для этого учтём, что в любом испытании случайный номер ячейки $n$ обязательно примет какое-либо(и только одно) значение $n = k$. Поэтому объединение всех событий, состоящих в попадании зерна в ячейку, есть достоверное событие. Вероятность достоверного события равна единице, а значит, суммарная вероятность всех возможных значений подчиняется условию нормировки 
\begin{align} \label{usl_normir}
	\sum_{k} p_k = 1
\end{align}

Для формулы \eqref{ver_formul} условие \eqref{usl_normir}, если (см.  пункт \ref{pril_theory} Приложение к теории)
\begin{align} \label{th:3:1}
	P(\overline{k}) = \frac{1}{\sqrt{2 \pi} \sigma_k}
\end{align}

Таким образом, стандарт $\sigma_k$ характеризует не только ширину, но и высоту холмика, описываемого формулой \eqref{ver_formul}.

Если в качестве случайной величины рассматривать не номер ячейки, а координату $x$, то дифференциальная функция распределения будет иметь вид
\begin{align} \label{dif_funk_raspred}
	W(x) = \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(- \frac{(x - \overline{x})^2}{2 \sigma^2}\right),
\end{align}
где $\overline{x}$ - координата средней ячейки, $\sigma = \sigma_k l$, $l$ - ширина ячейки. Функция \eqref{dif_funk_raspred} описывает нормальный закон распределения, или закон Гаусса.

Полная площадь под графиком $W(x)$ численно равна вероятности появления какого-нибудь значения $x$. и как вероятность достоверного события, она равна единице.
